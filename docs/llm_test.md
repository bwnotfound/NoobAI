在EleutherAI/gpt-neo-125m上测试微调效果时，发现了以下现象

构建数据集 26英文字母->蓝白bw ，batch_size设为2，数据集划分为50%训练，50%测试，学习率3e-6，
在训练8epoch后在测试集上基本记住了蓝白，在11epoch后在测试集上100%正确率。


对于数据集26英文字母->下一英文字母，batch_size设为2，数据集划分为50%训练，50%测试，学习率3e-6，
在训练5epoch后在测试集上100%正确率

对于数据集26英文字母->下一英文字母，batch_size设为2，数据集划分为50%训练，50%测试，学习率3e-5，
在训练2epoch后在测试集上100%正确率，12epoch后出现错误过拟合了

对于数据集26英文字母->下一英文字母，batch_size设为4，数据集划分为50%训练，50%测试，学习率3e-5，
始终无法做到100%正确率，准确率大概在2～3epoch取峰值

特别的，如果数据集构建的Instruction中没有指明预测（eg: input: "预测a->", output: "b"），则模型在测试集上不会收敛

结论：一定程度反映了llm的泛化能力的强大，以及所需epoch其实较少。学习率适中的情况下往往3epoch内最佳，其次是batch_size选择，这一点由于数据集较小，得不出准确结论，不过在微调方面考虑到数据集的成本，batch_size并非越大越好。